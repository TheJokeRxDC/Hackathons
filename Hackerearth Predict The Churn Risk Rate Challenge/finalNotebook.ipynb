{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackerearth Predict The Churn Risk Rate Challenge\n",
    "<hr>\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://d2908q01vomqb2.cloudfront.net/cb4e5208b4cd87268b208e49452ed6e89a68e0b8/2021/07/16/HackerEarthFeatureImage.png\" width=\"500\" height=\"600\">\n",
    "</p>\n",
    "\n",
    "## Problem\n",
    "\n",
    "Churn rate is a marketing metric that describes the number of customers who leave a business over a specific time period. . Every user is assigned a prediction value that estimates their state of churn at any given time. This value is based on:\n",
    "\n",
    "* User demographic information\n",
    "* Browsing behavior\n",
    "* Historical purchase data among other information\n",
    "\n",
    "It factors in our unique and proprietary predictions of how long a user will remain a customer. This score is updated every day for all users who have a minimum of one conversion. The values assigned are between 1 and 5.\n",
    "\n",
    "## Task\n",
    "\n",
    "Your task is to predict the churn score for a website based on the features provided in the dataset.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "<code> score = 100 x metrics.f1_score(actual, predicted, average=\"macro\") </code>\n",
    "\n",
    "Link : https://www.hackerearth.com/problem/machine-learning/predict-the-churn-risk-rate-11-fb7a760d/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# scikit-learn libraries/ML packages\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier, StackingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import lightgbm as lgb\n",
    "import sklearn.utils as sku\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the training and testing dataset\n",
    "df_train = pd.read_csv('data/train.csv', parse_dates=['joining_date'], na_values=['?','-999','Error','xxxxxxxx'])\n",
    "df_test = pd.read_csv('data/test.csv', parse_dates=['joining_date'], na_values=['?','-999','Error','xxxxxxxx'])\n",
    "sample = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "# Make a copy of the dataset\n",
    "df_train_copy = df_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy = df_train.copy()\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "enc = RobustScaler()\n",
    "label_enc = LabelEncoder()\n",
    "minmax_enc = MinMaxScaler()\n",
    "def preprocess(data, train=True):\n",
    "    '''\n",
    "    Step 1: Feature Extraction\n",
    "    Step 2: Dropping Columns\n",
    "    Step 3: Filling Null Values\n",
    "    Step 4: Converting to Categorica\n",
    "    '''\n",
    "    data['avg_frequency_login_days'] = pd.to_numeric(data['avg_frequency_login_days'], errors='coerce')\n",
    "    \n",
    "    # Step 1: Feature Extraction\n",
    "    data['days_joined'] = data['joining_date'].apply(lambda x:(pd.Timestamp('today') - x).days)\n",
    "    data['feedback'] = data['feedback'].apply(feedback_extract)\n",
    "    \n",
    "    # Step 2: Dropping the columns\n",
    "    if(train==True):\n",
    "        data = data[data['avg_time_spent'] > 0]\n",
    "        data = data[data['points_in_wallet'] >= 0]\n",
    "        data = data[data['churn_risk_score'] != -1]\n",
    "        target = data['churn_risk_score']\n",
    "        drop_cols = ['last_visit_time', 'joining_date', 'customer_id', 'Name', 'security_no', 'referral_id', 'churn_risk_score']\n",
    "        data.drop(drop_cols, axis=1, inplace=True)\n",
    "        \n",
    "        \n",
    "    elif(train==False):\n",
    "        cust_id = data['customer_id']\n",
    "        drop_cols = ['last_visit_time', 'joining_date', 'Name', 'security_no', 'referral_id', 'customer_id']\n",
    "        data.drop(drop_cols, axis=1, inplace=True)\n",
    "    \n",
    "    # Step 4: Converting to categorical columns and adding cat codes\n",
    "    for label, content in data.items():\n",
    "        # Converting string dtypes to categorical columns\n",
    "        if pd.api.types.is_string_dtype(content):\n",
    "            data[label] = content.astype(\"category\").cat.as_ordered()\n",
    "        \n",
    "        elif label in ['joining_quarter', 'joining_day', 'joining_year']:\n",
    "            data[label] = content.astype(\"category\").cat.as_ordered()\n",
    "    \n",
    "    # Step 3: Filling Null Values\n",
    "    mode_cols = ['region_category', 'joined_through_referral', 'preferred_offer_types', 'medium_of_operation']\n",
    "    data[mode_cols] = imp.fit_transform(data[mode_cols])\n",
    "        \n",
    "    median_col = ['points_in_wallet', 'avg_frequency_login_days', 'days_since_last_login']\n",
    "    data[median_col] = imputer.fit_transform(data[median_col])\n",
    "    \n",
    "    numerical = []\n",
    "    labels = []\n",
    "    \n",
    "    for label, content in data.items():\n",
    "        if not pd.api.types.is_string_dtype(content):\n",
    "            numerical.append(label)\n",
    "        else:\n",
    "            labels.append(label)\n",
    "            \n",
    "    data[numerical] = enc.fit_transform(data[numerical])\n",
    "    \n",
    "    for lab in labels:\n",
    "        data[lab] = label_enc.fit_transform(data[lab])\n",
    "\n",
    "    if train == True:\n",
    "        return data, target\n",
    "    elif train == False:\n",
    "        return data, cust_id\n",
    "\n",
    "def feedback_extract(data):\n",
    "    good_feed = ['Products always in Stock', 'Quality Customer Care', 'User Friendly Website', 'Reasonable Price']\n",
    "    bad_feed = ['Poor Website', 'Poor Product Quality', 'Poor Customer Service', 'Too many ads']\n",
    "    if data in good_feed:\n",
    "        return 'Positive'\n",
    "    elif data in bad_feed:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy, target = preprocess(df_train_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                             0\n",
       "gender                          0\n",
       "region_category                 0\n",
       "membership_category             0\n",
       "joined_through_referral         0\n",
       "preferred_offer_types           0\n",
       "medium_of_operation             0\n",
       "internet_option                 0\n",
       "days_since_last_login           0\n",
       "avg_time_spent                  0\n",
       "avg_transaction_value           0\n",
       "avg_frequency_login_days        0\n",
       "points_in_wallet                0\n",
       "used_special_discount           0\n",
       "offer_application_preference    0\n",
       "past_complaint                  0\n",
       "complaint_status                0\n",
       "feedback                        0\n",
       "days_joined                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_copy.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30855"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                             0\n",
       "gender                          0\n",
       "region_category                 0\n",
       "membership_category             0\n",
       "joined_through_referral         0\n",
       "preferred_offer_types           0\n",
       "medium_of_operation             0\n",
       "internet_option                 0\n",
       "days_since_last_login           0\n",
       "avg_time_spent                  0\n",
       "avg_transaction_value           0\n",
       "avg_frequency_login_days        0\n",
       "points_in_wallet                0\n",
       "used_special_discount           0\n",
       "offer_application_preference    0\n",
       "past_complaint                  0\n",
       "complaint_status                0\n",
       "feedback                        0\n",
       "days_joined                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_copy.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_experimentation(models, X_train, X_test, y_train, y_test):\n",
    "    '''\n",
    "    Fit and Score the deep learning models without performing hyperparameter tuning\n",
    "    '''\n",
    "    model_scores = {}\n",
    "    model_item = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        model_preds= model.predict(X_test)\n",
    "        model_scores[name] = 100 * f1_score(y_test, model_preds, average='macro')\n",
    "        model_item[name] = model\n",
    "    return model_scores, model_item\n",
    "\n",
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "\tscores = cross_val_score(model, X, y, scoring='f1_macro', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\treturn scores\n",
    "\n",
    "def test_prediction(model, X, name='submission.csv'):\n",
    "    predicted_df = pd.DataFrame()\n",
    "    X, predicted_df['customer_id'] = preprocess(X, False)\n",
    "    predicted_df['churn_risk_score'] = model.predict(X)\n",
    "    predicted_df.to_csv('data/'+name)\n",
    "\n",
    "def f1_eval(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    err = 1-f1_score(y_true, np.round(y_pred))\n",
    "    return 'f1_err', err\n",
    "\n",
    "def plot_features(columns, importances, n=20):\n",
    "    df = (pd.DataFrame({\"features\": columns,\n",
    "                        \"feature_importances\": importances})\n",
    "          .sort_values(\"feature_importances\", ascending=False)\n",
    "          .reset_index(drop=True))\n",
    "    # Plot the dataframe\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.barh(df[\"features\"][:n], df[\"feature_importances\"][:20])\n",
    "    ax.set_ylabel(\"Features\")\n",
    "    ax.set_xlabel(\"Feature importance\")\n",
    "    ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('data/test.csv', parse_dates=['joining_date'], na_values=['?','-999','Error','xxxxxxxx'])\n",
    "model_GBR = lgb.LGBMClassifier(objective='multi', random_state=42, n_jobs=-1)\n",
    "model_GBR.fit(df_train_copy, target)\n",
    "test_prediction(model_GBR, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x178046e4160>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "target.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "n_HP_points_to_test = 100\n",
    "clf = lgb.LGBMClassifier(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=-1, n_estimators=5000, objective='multi')\n",
    "param_test ={'num_leaves': sp_randint(6, 50), \n",
    "             'min_child_samples': sp_randint(100, 500), \n",
    "             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "             'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
    "             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n",
    "gs = RandomizedSearchCV(\n",
    "    estimator=clf, param_distributions=param_test, \n",
    "    n_iter=n_HP_points_to_test,\n",
    "    cv=3,\n",
    "    refit=True,\n",
    "    random_state=314,\n",
    "    verbose=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_train_copy, target, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed: 17.8min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 28.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score reached: 0.7940209764361037 with params: {'colsample_bytree': 0.6399212548086342, 'min_child_samples': 303, 'min_child_weight': 10.0, 'num_leaves': 10, 'reg_alpha': 10, 'reg_lambda': 50, 'subsample': 0.6346234410851537} \n"
     ]
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)\n",
    "print('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('data/test.csv', parse_dates=['joining_date'], na_values=['?','-999','Error','xxxxxxxx'])\n",
    "model_GBR =  lgb.LGBMClassifier(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=-1, n_estimators=5000, objective='multi',\n",
    "                               colsample_bytree=0.6399212548086342, min_child_samples=303, min_child_weight=10,\n",
    "                               num_leaves=10, reg_alpha=10, reg_lambda=50, subsample=0.6346234410851537)\n",
    "model_GBR.fit(df_train_copy, target)\n",
    "test_prediction(model_GBR, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
